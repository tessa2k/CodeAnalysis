{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9oBrNBsUCBa",
    "outputId": "ef9c1d12-db19-4673-c790-22ffe72bf028"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import io\n",
    "import zipfile\n",
    "import json\n",
    "import re\n",
    "import msal\n",
    "import requests\n",
    "import random\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load File to Local Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Azure application client info\n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')\n",
    "tenant_id = os.getenv('TENANT_ID')\n",
    "# redirect_uri = 'https://login.microsoftonline.com/common/oauth2/nativeclient'\n",
    "\n",
    "# Get access token\n",
    "authority = f'https://login.microsoftonline.com/{tenant_id}'\n",
    "scopes = ['Files.Read', 'User.Read', 'Files.ReadWrite']\n",
    "app = msal.PublicClientApplication(client_id, authority=authority)\n",
    "\n",
    "# Request token\n",
    "result = app.acquire_token_interactive(scopes=scopes)\n",
    "\n",
    "if \"access_token\" in result:\n",
    "    access_token = result[\"access_token\"]\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: 100603.zip - File ID: 01W2ZTQFSJAWW2GX2WYZBLNJYOCXJC6J4D\n",
      "File Name: 101629.zip - File ID: 01W2ZTQFQVNPI5KFYQFFD2NP2RFUSUPGB5\n",
      "File Name: 102279.zip - File ID: 01W2ZTQFRYTRTOQK7NYFGIP5FZYO4OQ2MO\n",
      "File Name: 102288.zip - File ID: 01W2ZTQFR3YOIAT2NXGJHKXGZTPIDCUXAL\n",
      "File Name: 102871.zip - File ID: 01W2ZTQFUPXK6KEU7KUBGLX5R22I4A3I4T\n",
      "File Name: 10360.zip - File ID: 01W2ZTQFXYZAPLRFDRMBBJZSVHNHGQ2Z4A\n",
      "File Name: 104623.zip - File ID: 01W2ZTQFWM3HZSVYJ36NFZYEZANTCJXNWS\n",
      "File Name: 105383.zip - File ID: 01W2ZTQFUWCPC7RW3A6BHLMKE7DPRG6QGE\n",
      "File Name: 105385.zip - File ID: 01W2ZTQFW5JTV62DBL2ZH3AML5TP6GNTP4\n",
      "File Name: 105501.zip - File ID: 01W2ZTQFTA2FMTPUEFR5ALKURMN75TIAS5\n",
      "File Name: 105506.zip - File ID: 01W2ZTQFQUS3USF2X3GVGYESXG2MQQBFS3\n",
      "File Name: 105507.zip - File ID: 01W2ZTQFQLDNT5AYOAQ5FLTZXPXHWQ54HX\n",
      "File Name: 105528.zip - File ID: 01W2ZTQFRGNFLGRWS3PJF2WDQOTSAXSIO4\n",
      "File Name: 106551.zip - File ID: 01W2ZTQFSWK7J6NGWGHFFLBVZAGZTOGOPO\n",
      "File Name: 106891.zip - File ID: 01W2ZTQFUWX4BGS2KQJ5DL4GTETQMRHEM4\n",
      "File Name: 108458.zip - File ID: 01W2ZTQFWZGUK6TJKCJRHZYDJTPYQ7SKLD\n",
      "File Name: 108459.zip - File ID: 01W2ZTQFQPZ27SG6YVVVCLVBCHJSCNYEQN\n",
      "File Name: 110022.zip - File ID: 01W2ZTQFXCT5BP6AIGTJEKKMDFUTPXVZGP\n",
      "File Name: 110023.zip - File ID: 01W2ZTQFT3ORAUVLFCIRBKMWKWF7OBS3IF\n",
      "File Name: 110560.zip - File ID: 01W2ZTQFX2C52KVRQOEZGYAKQ3QWOSZIEZ\n",
      "File Name: 111870.zip - File ID: 01W2ZTQFSB6MB3UZHKCNEYZIMZEFBHQ3JM\n",
      "File Name: 111877.zip - File ID: 01W2ZTQFQBR7443QIZ75EL4MZZ43YE6NBH\n",
      "File Name: 111880.zip - File ID: 01W2ZTQFQFJZIL2HXODNEYSHGO2ESUGKFV\n",
      "File Name: 111967.zip - File ID: 01W2ZTQFWZNLG3TVFUJVC2NTC2SA2FSGRV\n",
      "File Name: 112049.zip - File ID: 01W2ZTQFQKXHSTF32K2JE3IVSRQHOQ22AW\n",
      "File Name: 112071.zip - File ID: 01W2ZTQFVXMUZ6VDCXFZB3UMZ6CQ5G22L6\n",
      "File Name: 112079.zip - File ID: 01W2ZTQFV4R7GYXR6PFVFZIWFI3IPB2O5T\n",
      "File Name: 112086.zip - File ID: 01W2ZTQFRFR23G7AFC7VDJCUJERACZHRMZ\n",
      "File Name: 112348.zip - File ID: 01W2ZTQFU7DL7YAWUIOFF2RERYUQJ2LHVM\n",
      "File Name: 112349.zip - File ID: 01W2ZTQFRGKXC2IYM5TJALK4DLFWI64RGC\n",
      "File Name: 112359.zip - File ID: 01W2ZTQFRWP57LLV33OJFYC5MPGLVWLW4Q\n",
      "File Name: 112468.zip - File ID: 01W2ZTQFWY2OJQSLHGSBBZVJYYQOLBBGWD\n",
      "File Name: 112546.zip - File ID: 01W2ZTQFV5M6XNFDJ6JFDJL7UGYIPVRBJD\n",
      "File Name: 112547.zip - File ID: 01W2ZTQFVYGFFSZTMVHBAL6NHIVQD4YCNR\n",
      "File Name: 112633.zip - File ID: 01W2ZTQFR7DEVHSL4QKBELBOUEEDV7G4LP\n",
      "File Name: 112685.zip - File ID: 01W2ZTQFTJIMNUNIB2LJGJYBKLOEOZOXUV\n",
      "File Name: 112834.zip - File ID: 01W2ZTQFVZGIB2AHEHSNAZ2RA7WJLY7UDQ\n",
      "File Name: 112836.zip - File ID: 01W2ZTQFXADSI565J4XRAZFW5HHQAVWN4T\n",
      "File Name: 112914.zip - File ID: 01W2ZTQFXQ73UYHURFNFH3GZDEV7CUU6IH\n",
      "File Name: 112915.zip - File ID: 01W2ZTQFTBR5QBAANMV5HZAI5JCW2BGWAV\n",
      "File Name: 112922.zip - File ID: 01W2ZTQFXQ44IBCM526VAYSK6WBHWIN23X\n",
      "File Name: 112923.zip - File ID: 01W2ZTQFSM6EKG57SVCVELEVUABTH6CZ3B\n",
      "File Name: 112968.zip - File ID: 01W2ZTQFR4BFYTUQS7ARBLVXDLGJ74SIOV\n",
      "File Name: 113426.zip - File ID: 01W2ZTQFWRXEQ6S5G37FHIGAFXMADBDGEM\n",
      "File Name: 113435.zip - File ID: 01W2ZTQFQ32VHZHW3Q6RD3ESXNHP4KHOLT\n",
      "File Name: 113446.zip - File ID: 01W2ZTQFQMUZCLAU6GNRCIIG4N5TILRGEI\n",
      "File Name: 113459.zip - File ID: 01W2ZTQFVEVP3ANT6FYNELOPZBDXKGTKW3\n",
      "File Name: 113649.zip - File ID: 01W2ZTQFTRNHQGVPMMDVBILSRIW7RJB42A\n",
      "File Name: 113732.zip - File ID: 01W2ZTQFQXS5CQI35PGND27OZNUFSNYK4Y\n",
      "File Name: 113939.zip - File ID: 01W2ZTQFWWODO6H2FJCFCYJRZ7H4CBA7PI\n",
      "File Name: 113949.zip - File ID: 01W2ZTQFVMZC4IJYPA4RBYWZMWWAFIL4RC\n",
      "File Name: 113997.zip - File ID: 01W2ZTQFSRNPHCAKLHNJFKJSFBWEC3XDSA\n",
      "File Name: 114047.zip - File ID: 01W2ZTQFTFLQMRYGOXERBIBLFYO6VXD3OT\n",
      "File Name: 114108.zip - File ID: 01W2ZTQFUHNQEFNC5DX5DK2O657KURC3BX\n",
      "File Name: 114230.zip - File ID: 01W2ZTQFWQ5XRJDLOIYFEKP7LOC7JVYK5T\n",
      "File Name: 114242.zip - File ID: 01W2ZTQFUHGAPSQZTQXZB3QG55MLKLCFS5\n",
      "File Name: 114310.zip - File ID: 01W2ZTQFQCKQV7EXDUEZBYUJ2DBMOYKB5L\n",
      "File Name: 114337.zip - File ID: 01W2ZTQFWAM2ZVPCFMPBFZECDDBRU5BXF2\n",
      "File Name: 114342.zip - File ID: 01W2ZTQFSY2TDFPOJ4IVCI6RQ7CWCOIHAK\n",
      "File Name: 114355.zip - File ID: 01W2ZTQFS6K2X3YR6V7FEJEBU6MWITCTII\n",
      "File Name: 114359.zip - File ID: 01W2ZTQFUXJ75RX2LPRBB2OYFNAFHZWOT5\n",
      "File Name: 114365.zip - File ID: 01W2ZTQFTCLDTIIK4CDJCYJ752SAZRWSGB\n",
      "File Name: 114394.zip - File ID: 01W2ZTQFXL4DU65XLJQVELBJAX4QBHSPZQ\n",
      "File Name: 114424.zip - File ID: 01W2ZTQFTOXL5LLRNL2RGK3QINA2DFGSSH\n",
      "File Name: 114450.zip - File ID: 01W2ZTQFVN4W3PBA3Q6BBLN7CKRAAQHI26\n",
      "File Name: 114451.zip - File ID: 01W2ZTQFSE7UXLNFPVGBH26LWM3C7SZ56K\n",
      "File Name: 114452.zip - File ID: 01W2ZTQFRNQTR65HAVFRD3TORBTMIT5O4T\n",
      "File Name: 114637.zip - File ID: 01W2ZTQFTQMGGYQX2QRJCYFHFERGIW5IY2\n",
      "File Name: 114639.zip - File ID: 01W2ZTQFSI5GCXUW2V7BHY2QBRHP5KLLEC\n",
      "File Name: 114643.zip - File ID: 01W2ZTQFTXAVFQHI5XBFGZFEPLW4CLJKWP\n",
      "File Name: 114652.zip - File ID: 01W2ZTQFQXW2XPH3LGZVHZZVIBUGI7SJ67\n",
      "File Name: 114653.zip - File ID: 01W2ZTQFX4XR6TZ7YGNZB2SI2PTILWQRPO\n",
      "File Name: 114654.zip - File ID: 01W2ZTQFQ2DYOSMXNRQZDKA42VVKHG7VKG\n",
      "File Name: 114655.zip - File ID: 01W2ZTQFTO7DCNJ4ZKRZFIJMISF2FZRFUQ\n",
      "File Name: 114665.zip - File ID: 01W2ZTQFV62CO6IMXJPNEKSWEC4WBA3JHO\n",
      "File Name: 114685.zip - File ID: 01W2ZTQFRK4V64OMKAANBJI7AJY4MSWUH5\n",
      "File Name: 114735.zip - File ID: 01W2ZTQFTKVKXOYMQEYBAIZ3IAD6VD6BWM\n",
      "File Name: 115356.zip - File ID: 01W2ZTQFSQ2UWXJHDTQ5EJX53NAXHS26NT\n",
      "File Name: 115357.zip - File ID: 01W2ZTQFXZQSZMMU33FFDL6IZUDHJHUVYF\n",
      "File Name: 115435.zip - File ID: 01W2ZTQFRKX7DGJOOO7BEY4GKIS33VZJOM\n",
      "File Name: 115813.zip - File ID: 01W2ZTQFTHQ6ZPSNXW5JC3HDC5SD2TVQD3\n",
      "File Name: 115920.zip - File ID: 01W2ZTQFWJWPQJY5YGIVHLRTY7F2JN7MRC\n",
      "File Name: 115966.zip - File ID: 01W2ZTQFSDKIBRXBU64JGJG6YMIVBVSGNK\n",
      "File Name: 115968.zip - File ID: 01W2ZTQFVUARKVYGZPURDJLCB3E2BTU2EG\n",
      "File Name: 116053.zip - File ID: 01W2ZTQFTAFOOOT6AMIZBKNLTZPNQBW3WF\n",
      "File Name: 116079.zip - File ID: 01W2ZTQFROUKQEDVEV2JH2OYU2HOA76B7S\n",
      "File Name: 116080.zip - File ID: 01W2ZTQFQ3CC5FBMT7M5D2V44ABKLSV6HS\n",
      "File Name: 116081.zip - File ID: 01W2ZTQFTSQ7LGBU74CVH3NQXQLO6SX2CJ\n",
      "File Name: 116082.zip - File ID: 01W2ZTQFS7ZG5MCTGZCVEZ7PAW7WBMPBP5\n",
      "File Name: 116083.zip - File ID: 01W2ZTQFWILZVCEVAUWZHYRVPO5EUAMETD\n",
      "File Name: 116084.zip - File ID: 01W2ZTQFSXXLTXZZ35TZGYYR4IX6F3APAV\n",
      "File Name: 116086.zip - File ID: 01W2ZTQFVJXCZW74ZNIBAI27KJZTDMGEM5\n",
      "File Name: 116094.zip - File ID: 01W2ZTQFSTRO4RRXBUKNAIILUOYXT4IFRX\n",
      "File Name: 116096.zip - File ID: 01W2ZTQFVPTRQ4CK6PHJCZAN42VC4PQJV4\n",
      "File Name: 116123.zip - File ID: 01W2ZTQFSVPYQD5Y3IIFD2FWHU356PCOTM\n",
      "File Name: 116312.zip - File ID: 01W2ZTQFTZVCNDHKUMFFC3PR2S74HO6PXH\n",
      "File Name: 116313.zip - File ID: 01W2ZTQFSZKBGGN6RRERDZWYB7GHNYHXIW\n",
      "File Name: 116368.zip - File ID: 01W2ZTQFWB2CZFDTHTZZE337XA2AQRVWQG\n",
      "File Name: 116386.zip - File ID: 01W2ZTQFTG5TG3FLJY7VEZGKDYS2MNDJ7Y\n",
      "File Name: 116491.zip - File ID: 01W2ZTQFQOSVM4VYQGKZCLAGDTPZJXEUKL\n",
      "File Name: 116567.zip - File ID: 01W2ZTQFWHSWC3RCYH7FCJFCSTBFO42XK6\n",
      "File Name: 116575.zip - File ID: 01W2ZTQFTPVJHZPBYWPVELQVV3HNSLNSCT\n",
      "File Name: 116606.zip - File ID: 01W2ZTQFXGMIFJKR4QUVAZVDBZCQ74CX2Z\n",
      "File Name: 116740.zip - File ID: 01W2ZTQFVMOOKJGUYUQRFKXKP3D3TV57XJ\n",
      "File Name: 116769.zip - File ID: 01W2ZTQFSYW7HXAMKZZRAJYVTRNGB2P235\n",
      "File Name: 116806.zip - File ID: 01W2ZTQFUB6MIUO4FO4JHLINSZWKNP6VMS\n",
      "File Name: 116830.zip - File ID: 01W2ZTQFVJ2TKVP4JCOVCKTXSKPVNUZ5GF\n",
      "File Name: 116832.zip - File ID: 01W2ZTQFSVTFSU7K3GDFA3XJOOX34OK5CA\n",
      "File Name: 116835.zip - File ID: 01W2ZTQFW255MR2BMK7JGLVXFOSCXD7H3Y\n",
      "File Name: 116837.zip - File ID: 01W2ZTQFQ5AUKUQUMYTNHLGBTMU4JENY4B\n",
      "File Name: 116838.zip - File ID: 01W2ZTQFUTMGHGF6VILVFYF6JAGRPHOSFL\n",
      "File Name: 116859.zip - File ID: 01W2ZTQFRC624PTZTW75CK3XEGWPVOGS6L\n",
      "File Name: 116860.zip - File ID: 01W2ZTQFSDAFUZYX26KNB2P2BGC7VPJEP5\n",
      "File Name: 116862.zip - File ID: 01W2ZTQFUSROQGM7YYXJAZA27EGKJRBSN7\n",
      "File Name: 116867.zip - File ID: 01W2ZTQFU2RPHILHF6TZAZDVLCX6V7ED7U\n",
      "File Name: 116870.zip - File ID: 01W2ZTQFQIWGFOAM3KU5F3P6QFGL4KQAUL\n",
      "File Name: 116871.zip - File ID: 01W2ZTQFSFNK3VRMKTVRGLEPFQSYZEUIWZ\n",
      "File Name: 116901.zip - File ID: 01W2ZTQFURVCNTTMMRFZE2XFXOKO4NR464\n",
      "File Name: 116942.zip - File ID: 01W2ZTQFS6F7HGZ4NNPZFL5TB232QMQ6U6\n",
      "File Name: 116945.zip - File ID: 01W2ZTQFWTOLJ5KRRPAJCZ273CN3ARTASF\n",
      "File Name: 116956.zip - File ID: 01W2ZTQFU2EFRUW3QHGBEKXDPEE3AAG4ZW\n",
      "File Name: 116957.zip - File ID: 01W2ZTQFXQLDZAE6CHEFAIY5TSX6GHLMUX\n",
      "File Name: 116981.zip - File ID: 01W2ZTQFUVZWCGSPUKGZH3C5VR2YNP7X6K\n",
      "File Name: 116983.zip - File ID: 01W2ZTQFT6WGPXHUZTR5BYIRGVUBBJCSX7\n",
      "File Name: 117204.zip - File ID: 01W2ZTQFQQUNDII5WMKNAIMMIDXS3XDO45\n",
      "File Name: 117207.zip - File ID: 01W2ZTQFQKEA2QBTCARJAJJZRL7KZFPRMS\n",
      "File Name: 117330.zip - File ID: 01W2ZTQFQCATYX4M2AXZAJIC2ERZTTWKHK\n",
      "File Name: 117351.zip - File ID: 01W2ZTQFTBOPK5M3HK2NGLYQO7CFEUFPPP\n",
      "File Name: 117359.zip - File ID: 01W2ZTQFSO7SXHENKPVJB3KAFILVJUNWEI\n",
      "File Name: 117361.zip - File ID: 01W2ZTQFX4GK2UBOWOPVEJ3N3ADP2GV2JZ\n",
      "File Name: 117459.zip - File ID: 01W2ZTQFRQHN3J2I2IF5FIHAXQ7YHMJSGR\n",
      "File Name: 117508.zip - File ID: 01W2ZTQFWOUQCJPYRLQJBYBGOG34E43FHC\n",
      "File Name: 117514.zip - File ID: 01W2ZTQFV5ALF52KKAZFEJADCHQT67GP25\n",
      "File Name: 117691.zip - File ID: 01W2ZTQFRQMPDPZQYNHJGJLISTUPGZA4HG\n",
      "File Name: 117810.zip - File ID: 01W2ZTQFRIDEDRHG2LNNDZ72CMZEOLIMR2\n",
      "File Name: 117811.zip - File ID: 01W2ZTQFWOXRQWJ6GSUVBJN6XHLLLS2VVI\n",
      "File Name: 117966.zip - File ID: 01W2ZTQFU6Y6LNXQWCLBAY27IVPH4DKWA4\n",
      "File Name: 118014.zip - File ID: 01W2ZTQFUGMWOXEE6SIZD3V67DCK2ASFTU\n",
      "File Name: 118020.zip - File ID: 01W2ZTQFWC445TCESUPRA3AQVI7Y36FVDE\n",
      "File Name: 118091.zip - File ID: 01W2ZTQFTOBT36BR6JJ5BKA7UN4YG46HVP\n",
      "File Name: 118092.zip - File ID: 01W2ZTQFUSYU7LDQ6F3BC3DS3PPN4MOYCP\n",
      "File Name: 118098.zip - File ID: 01W2ZTQFQ7M6FHTVYGQVH3K5HN6FPFZVFW\n",
      "File Name: 118195.zip - File ID: 01W2ZTQFRQAQ3W4SZYKREYHTSQKMRSPE5U\n",
      "File Name: 118196.zip - File ID: 01W2ZTQFQ7IRYW6M2XWZHYD25IMGWB4UC6\n",
      "File Name: 118199.zip - File ID: 01W2ZTQFSYT5SORIXGBRAYTNFWUGIN65GA\n",
      "File Name: 118261.zip - File ID: 01W2ZTQFXDZGVV47GLPRHLCYAMF6L6PR2J\n",
      "File Name: 118326.zip - File ID: 01W2ZTQFXXPBMY6GBR4BHIF3HZIV5RXKTB\n",
      "File Name: 118389.zip - File ID: 01W2ZTQFQWPEPSW5U4KJDK7BBZYALUHJPE\n",
      "File Name: 118392.zip - File ID: 01W2ZTQFRNJKQYM56QIZBLHJRFDXH6GYRP\n",
      "File Name: 118434.zip - File ID: 01W2ZTQFRIDQRRRLFWOJHYV64NUX6RV2VX\n",
      "File Name: 118518.zip - File ID: 01W2ZTQFQBCJV23NV4YFCZNQF666AAJYVR\n",
      "File Name: 118524.zip - File ID: 01W2ZTQFX3TBP5XHCM7JBZDJTBSO6CWZZI\n",
      "File Name: 118528.zip - File ID: 01W2ZTQFQS3OJIHXPOVZAZ4IPLM27LM5CI\n",
      "File Name: 118554.zip - File ID: 01W2ZTQFS4UQPMUSMB7NFLVZRDQSMS66BV\n",
      "File Name: 118631.zip - File ID: 01W2ZTQFRHHBBJCNKEYZHKYTNPG6OPQ45O\n",
      "File Name: 118662.zip - File ID: 01W2ZTQFWANSHHLWXVKRHZXBUCGKIR76ZE\n",
      "File Name: 118759.zip - File ID: 01W2ZTQFVYEHY5SSN44VCYXRLRXCBIAMXJ\n",
      "File Name: 118797.zip - File ID: 01W2ZTQFQCV52B7UZDBFHZLEZSS7Q5WEN6\n",
      "File Name: 118799.zip - File ID: 01W2ZTQFTNIAFQQ4LMPZBKG35HK4RIVJ4J\n",
      "File Name: 118894.zip - File ID: 01W2ZTQFXAARP4NUPPD5CJPH65QQ3BSRG4\n",
      "File Name: 118986.zip - File ID: 01W2ZTQFUXK4S73NTLWBFJWAB7CEGC667L\n",
      "File Name: 119153.zip - File ID: 01W2ZTQFQBMJWUDG6IIFB3VEPE7LBGEHKY\n",
      "File Name: 119159.zip - File ID: 01W2ZTQFUHGCWP3ICBDND3FY2YPU2VR2L3\n",
      "File Name: 119214.zip - File ID: 01W2ZTQFWZJBCH3PMB6ZGLYUV273IRZ3ZU\n",
      "File Name: 119266.zip - File ID: 01W2ZTQFQFWEOVHCRKT5AZIXRBK5JWSQSI\n",
      "File Name: 119283.zip - File ID: 01W2ZTQFS2P633VGEUTBEL7QCRTCTJAWJF\n",
      "File Name: 120115.zip - File ID: 01W2ZTQFWN3USKROHQZ5FZ2D2TVSZGREOQ\n",
      "File Name: 120117.zip - File ID: 01W2ZTQFUA6UNBJADF45FZVD47JPHLQDV3\n",
      "File Name: 120137.zip - File ID: 01W2ZTQFWWFAOER6AIEZCKG7F7FEYR2PLI\n",
      "File Name: 120184.zip - File ID: 01W2ZTQFWJCTK7B5VCCNA3J4PHYMVR6YIQ\n",
      "File Name: 120227.zip - File ID: 01W2ZTQFXVZIAHAPDFQ5A337VRUE2ZXYNW\n",
      "File Name: 120243.zip - File ID: 01W2ZTQFSJZWYBUAL2DNFKROGYE46AYX2B\n",
      "File Name: 120246.zip - File ID: 01W2ZTQFTPAYNFZE5Y3NALW73VTNRBUQDB\n",
      "File Name: 120320.zip - File ID: 01W2ZTQFTYSS6SN4DFJRALMSPHSFVLB5NL\n",
      "File Name: 120325.zip - File ID: 01W2ZTQFVTJ6AUNPHT4REYNI4SFTEWU54D\n",
      "File Name: 120521.zip - File ID: 01W2ZTQFWIN7PYMJING5BK2HQ44OLJ3E4V\n",
      "File Name: 120692.zip - File ID: 01W2ZTQFTMTAXTPMOKJRGZ2GJSK33OCERF\n",
      "File Name: 120783.zip - File ID: 01W2ZTQFTHIN6V7XOAYRBLSQFA6TD44APM\n",
      "File Name: 120798.zip - File ID: 01W2ZTQFTGCUJNSE6KBRC25DQO7PQFQHCH\n",
      "File Name: 120835.zip - File ID: 01W2ZTQFR4TUG65FBV25HI3CNRPQM2NNVD\n",
      "File Name: 120907.zip - File ID: 01W2ZTQFQMZC5SGJQ2W5CYC3MIPCLJAPQG\n",
      "File Name: 120910.zip - File ID: 01W2ZTQFSQDKM7WFXWEZAKZOQWO57JEVAT\n",
      "File Name: 121060.zip - File ID: 01W2ZTQFXVPBLSQD5SXRBLPJJF4X67QDL6\n",
      "File Name: 121253.zip - File ID: 01W2ZTQFTY65YASZT76ZEJR5M4RYVQBHZB\n",
      "File Name: 121259.zip - File ID: 01W2ZTQFV5Z75XGKBCZBAZDSXLQXYQ34MY\n",
      "File Name: 121600.zip - File ID: 01W2ZTQFRBUHJPQHGWJRELRUYQAFQZQVMX\n",
      "File Name: 121628.zip - File ID: 01W2ZTQFWTVBEC3AXELJCLG2OB2CZICLG2\n",
      "File Name: 122329.zip - File ID: 01W2ZTQFWW3IRZZX3LSNHLD3XSU3G7S7N4\n",
      "File Name: 122369.zip - File ID: 01W2ZTQFR66UDBESKD5RFKBJSCQCMRB65K\n",
      "File Name: 122442.zip - File ID: 01W2ZTQFXLW7NM2B5KO5C3GGRGSGTRQNWS\n",
      "File Name: 123086.zip - File ID: 01W2ZTQFUHFA2AMTPMBFBJ6BMY4PKXCR4X\n",
      "File Name: 123453.zip - File ID: 01W2ZTQFQFN5YKWBAYVJD2E2JGLH7E4HUJ\n",
      "File Name: 123623.zip - File ID: 01W2ZTQFRBCQTOLIQQVRCZKWFQVOFBE566\n",
      "File Name: 123815.zip - File ID: 01W2ZTQFXMEHSGMKYFTBFKVK5YEFOPNNK2\n",
      "File Name: 123848.zip - File ID: 01W2ZTQFW73T6JECRDI5F24D2CRUJNAGZ2\n",
      "File Name: 123897.zip - File ID: 01W2ZTQFXZIRWDJQAS5FHYPP5ET5FXPHGG\n",
      "File Name: 123927.zip - File ID: 01W2ZTQFVVPGDTQMMVEFG266KA4XALVFJC\n",
      "File Name: 123928.zip - File ID: 01W2ZTQFQMXJHKDMTEXZA33JYDI6UWB2CA\n",
      "File Name: 123986.zip - File ID: 01W2ZTQFRNGL7UXH7EXFBYWZDHPBUVKWJD\n",
      "File Name: 123987.zip - File ID: 01W2ZTQFV2F5GPDE3B55CKEPMKGXXJ6JAI\n"
     ]
    }
   ],
   "source": [
    "# Access \"modeldb-code-analysis/modeldb-zips\"\n",
    "endpoint = 'https://graph.microsoft.com/v1.0/me/drive/root:/modeldb-code-analysis/modeldb-zips:/children'\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "file_code_id = {}\n",
    "if response.status_code == 200:\n",
    "    files_in_subfolder = response.json().get('value', [])\n",
    "    for file in files_in_subfolder:\n",
    "        file_code = file['name'][:-4]\n",
    "        file_code_id[file_code] = file['id']\n",
    "        print(f\"File Name: {file['name']} - File ID: {file['id']}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 114665.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/114665.zip\n",
      "Unzip 114665.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/114665\n",
      "Downloaded 118434.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/118434.zip\n",
      "Unzip 118434.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/118434\n",
      "Downloaded 114424.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/114424.zip\n",
      "Unzip 114424.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/114424\n",
      "Downloaded 105383.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/105383.zip\n",
      "Unzip 105383.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/105383\n",
      "Downloaded 113949.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/113949.zip\n",
      "Unzip 113949.zip to /Users/tessakong/Desktop/CodeAnalysis/sample5/113949\n"
     ]
    }
   ],
   "source": [
    "# get shuffled file code\n",
    "random.seed(20)\n",
    "file_code_list = list(file_code_id.keys())\n",
    "random.shuffle(file_code_list)\n",
    "\n",
    "sample_folder = '/Users/tessakong/Desktop/CodeAnalysis/sample5'\n",
    "os.makedirs(sample_folder, exist_ok=True)\n",
    "\n",
    "for code in file_code_list[:5]:\n",
    "    file_id = file_code_id[code]\n",
    "    zip_filename = f\"{code}.zip\"\n",
    "    local_path = os.path.join(sample_folder, zip_filename)\n",
    "    extract_path = os.path.join(sample_folder, code)\n",
    "\n",
    "    # download zip file into local directory\n",
    "    download_endpoint = f'https://graph.microsoft.com/v1.0/me/drive/items/{file_id}/content'\n",
    "    response = requests.get(download_endpoint, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(local_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded {zip_filename} to {local_path}\")\n",
    "        \n",
    "        with zipfile.ZipFile(local_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(f\"Unzip {zip_filename} to {extract_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_folder(path, score_metric, acceptable_extensions, pattern_mapping):\n",
    "    for entry in os.listdir(path):\n",
    "        full_path = os.path.join(path, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            print(f'Traverse folder: {full_path}')\n",
    "            traverse_folder(full_path, score_metric, acceptable_extensions, pattern_mapping)\n",
    "        else:\n",
    "            # Check if the file extension is acceptable\n",
    "            if not entry.lower().endswith(acceptable_extensions):\n",
    "                continue  # Skip the file if the extension is not acceptable\n",
    "            score = 0   \n",
    "            with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                for pattern in pattern_mapping.keys():\n",
    "                    if pattern.search(content):  # If any rule matches the file content\n",
    "                        score += 1\n",
    "            score_metric.append((score, full_path))\n",
    "                                      \n",
    "\n",
    "\n",
    "def get_score_metric(model_name, json_file_path, sample_folder = '/Users/tessakong/Desktop/CodeAnalysis/sample5'):\n",
    "    extract_folder = f'{sample_folder}/{model_name}'\n",
    "    score_metric = []\n",
    "    # Load rules from the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        rules = json.load(json_file)\n",
    "    # Convert rules to a dictionary of regex patterns and replacements\n",
    "    pattern_mapping = {re.compile(pattern): replacement for pattern, replacement in rules.items()}\n",
    "    matched_files = [] \n",
    "    # Define acceptable file extensions\n",
    "    acceptable_extensions = ('.py', '.cpp', '.java', '.m', '.txt', '.h', '.data', \n",
    "                             '.html', '.c', '.mod', '.g', '.p', \".ode\", \".html\")  # Adjust as needed\n",
    "\n",
    "    traverse_folder(extract_folder, score_metric, acceptable_extensions, pattern_mapping)\n",
    "    return score_metric\n",
    "\n",
    "def concat_files(code, sample_folder, file_path_list, topK):\n",
    "    output_file_folder = f'{sample_folder}/match_file'\n",
    "    os.makedirs(output_file_folder, exist_ok=True)\n",
    "    output_file_path = f'{output_file_folder}/{code}_top{topK}.txt'\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for file_path in file_path_list:\n",
    "            output_file.write(f'=== {file_path} ===\\n')  # Write the file path\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                output_file.write(f.read())  # Write the file content\n",
    "                output_file.write('\\n\\n')  # Add a newline between files\n",
    "\n",
    "    print(f\"Concatenated file for model {code} have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================Processing model 114665==============================\n",
      "Traverse folder: /Users/tessakong/Desktop/CodeAnalysis/sample5/114665/plast\n",
      "6\n",
      "Concatenated file for model 114665 have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/114665_top6.txt\n",
      "==============================Processing model 118434==============================\n",
      "Traverse folder: /Users/tessakong/Desktop/CodeAnalysis/sample5/118434/KulviciusEtAl2008\n",
      "3\n",
      "Concatenated file for model 118434 have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/118434_top3.txt\n",
      "==============================Processing model 114424==============================\n",
      "Traverse folder: /Users/tessakong/Desktop/CodeAnalysis/sample5/114424/LampreyNMDAosc\n",
      "1\n",
      "Concatenated file for model 114424 have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/114424_top1.txt\n",
      "==============================Processing model 105383==============================\n",
      "Traverse folder: /Users/tessakong/Desktop/CodeAnalysis/sample5/105383/GPCv1.0.8\n",
      "Traverse folder: /Users/tessakong/Desktop/CodeAnalysis/sample5/105383/GPCv1.0.8/cvode\n",
      "29\n",
      "Concatenated file for model 105383 have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/105383_top29.txt\n",
      "==============================Processing model 113949==============================\n",
      "Traverse folder: /Users/tessakong/Desktop/CodeAnalysis/sample5/113949/gnrh_passdend_simfiles\n",
      "Traverse folder: /Users/tessakong/Desktop/CodeAnalysis/sample5/113949/gnrh_passdend_simfiles/output\n",
      "5\n",
      "Concatenated file for model 113949 have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/113949_top5.txt\n"
     ]
    }
   ],
   "source": [
    "json_file_path = \"/Users/tessakong/Desktop/CodeAnalysis/manual_classifier_rules.json\"\n",
    "for code in file_code_list[:5]:\n",
    "    print(f'==============================Processing model {code}==============================')\n",
    "    file_id = file_code_id[code]\n",
    "    scores = get_score_metric(code, json_file_path, sample_folder)\n",
    "    scores.sort(key = lambda x: x[0], reverse = True)\n",
    "    propotion = 0.5\n",
    "    topK = int(propotion * len(scores))\n",
    "    print(topK)\n",
    "    if topK == 0:\n",
    "        print(f\"topK is 0 for model {code}, ignore\")\n",
    "        continue\n",
    "    file_path_list = [s[1] for s in scores[:topK]]\n",
    "    concat_files(code, sample_folder, file_path_list, topK)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Generation (currents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request(url, method = 'GET', headers=None, params=None, json_data=None):\n",
    "    '''\n",
    "    Parameters:\n",
    "      - url (str): The API endpoint.\n",
    "      - method (str): The HTTP method ('GET', 'POST', etc.). Default is 'GET'.\n",
    "      - headers (dict): Optional headers for the request.\n",
    "      - params (dict): Optional URL parameters for the request.\n",
    "      - json_data (dict): Optional JSON data for POST requests.\n",
    "\n",
    "      Returns:\n",
    "      - response (dict): Parsed JSON response from the API.\n",
    "    '''\n",
    "    url = \"https://modeldb.science/\" + url\n",
    "    try:\n",
    "        # Determine the request method\n",
    "        if method.upper() == 'GET':\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "        elif method.upper() == 'POST':\n",
    "            response = requests.post(url, headers=headers, json=json_data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported HTTP method: {}\".format(method))\n",
    "\n",
    "        # Check for HTTP errors\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse JSON response\n",
    "        return response.json()\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Error occurred: {req_err}\")\n",
    "    except ValueError as json_err:\n",
    "        print(f\"JSON decode error: {json_err}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['celltypes', 'currents', 'genes', 'modelconcepts', 'models', 'modeltypes', 'papers', 'receptors', 'regions', 'simenvironments', 'transmitters']\n"
     ]
    }
   ],
   "source": [
    "cat_url = \"/api/v1/\"\n",
    "metadata_categories = api_request(cat_url, method = 'GET')\n",
    "print(metadata_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'celltypes': 278,\n",
      " 'currents': 64,\n",
      " 'genes': 62,\n",
      " 'modelconcepts': 214,\n",
      " 'models': 1879,\n",
      " 'modeltypes': 21,\n",
      " 'papers': 81024,\n",
      " 'receptors': 60,\n",
      " 'regions': 47,\n",
      " 'simenvironments': 174,\n",
      " 'transmitters': 25}\n"
     ]
    }
   ],
   "source": [
    "count_metadata = {}\n",
    "for e in metadata_categories:\n",
    "    temp_url = cat_url + e\n",
    "    count_metadata[e] = len(api_request(temp_url, method = 'GET'))\n",
    "pprint.pprint(count_metadata)\n",
    "model_url = \"/api/v1/currents/name\"\n",
    "model_current_list = api_request(model_url, method = 'GET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================model 118434 has 42906 tokens===================\n",
      "Generated metadata: I Na,t, I A, I K\n",
      "===================model 105383 has 630400 tokens===================\n",
      "Generated metadata: I Na, INa, I Calcium, I Calcium, I_Ks, Na/Ca exchanger, Na/K pump\n",
      "===================model 114424 has 1606 tokens===================\n",
      "Generated metadata: I K,Ca, I K, I K,leak, I Calcium\n",
      "===================model 114665 has 2645986 tokens===================\n",
      "Generated metadata: I Na,p, I Na,t, I K, I Sodium, I Potassium\n",
      "===================model 113949 has 60036 tokens===================\n",
      "Generated metadata: I K\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv('API_KEY')\n",
    "organization=os.getenv('ORGANIZATION')\n",
    "\n",
    "match_file_folder = '/Users/tessakong/Desktop/CodeAnalysis/sample5/match_file'\n",
    "save_prompt = \"You are a neuroscience expert specializing in ion channel and current analysis. Given the following content: please identify the most relevant ion currents from the following list. The list includes: ['I Chloride', 'I Na,p', 'I Na,t', 'I L high threshold', 'I N', 'I T low threshold', 'I p,q', 'I A', 'I K', 'I K,leak', 'I M', 'I h', 'I Cl,Ca', 'I K,Ca', 'I CNG', 'I CAN', 'I Sodium', 'I Calcium', 'I Mixed', 'I Potassium', 'I A, slow', 'ATP-sensitive potassium current', 'I_KHT', 'I_KLT', 'I_HERG', 'Late Na', 'Na/Ca exchanger', 'I_Na,Ca', 'I_SERCA', 'KCNQ1', 'I_Ks', 'I Krp', 'I R', 'I Q', 'I_K,Na', 'Na/K pump', 'I_AHP', 'I ANO2', 'I trp', 'I Cl, leak', 'I Na, leak', 'I Ca,p', 'I_KD', 'Osmosis-driven water flux', 'KCC2', 'NKCC1', 'Ca pump', 'I_HCO3', 'Channelrhodopsin (ChR)', 'Kir', 'I MI', 'I TRPM8', 'Kir2 leak', 'I Na, slow inactivation', 'Na+-glutamate transporter', 'IK Bkca', 'I Ca SOCC', 'IK Skca', 'KCC1', 'NBC', 'I C', 'TASK channel', 'Kir, inactivating', 'I TRPM4'].\\n\\nBased on this content, please provide the most relevant ion current(s), and list them separated by commas.\"\n",
    "\n",
    "output_json_path = os.path.join(sample_folder, \"prompt_test.json\")\n",
    "# if os.path.exists(output_json_path):\n",
    "#     with open(output_json_path, 'r', encoding='utf-8') as json_file:\n",
    "#         output_data = json.load(json_file)\n",
    "#         if not isinstance(output_data, list):\n",
    "#             output_data = []\n",
    "# else:\n",
    "#     output_data = []\n",
    "output_data = []\n",
    "\n",
    "new_prompt_block = {\n",
    "    \"prompt\": save_prompt,\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "client = OpenAI(api_key=api_key, organization=organization) \n",
    "\n",
    "for entry in os.listdir(match_file_folder):\n",
    "    full_path = os.path.join(match_file_folder, entry)\n",
    "    code = entry.split('_')[0]\n",
    "\n",
    "    with open(full_path, 'r', encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "        print(f'===================model {code} has {len(file_content)} tokens===================')\n",
    "        file_content = file_content[:5000]\n",
    "    prompt = (\n",
    "    f\"You are a neuroscience expert specializing in ion channel and current analysis. \"\n",
    "    f\"Given the following content:\\n\\n{file_content}\\n\\n\"\n",
    "    f\"Please identify the most relevant ion currents from the following list. \"\n",
    "    f\"The list includes: ['I Chloride', 'I Na,p', 'I Na,t', 'I L high threshold', 'I N', 'I T low threshold', 'I p,q', \"\n",
    "    f\"'I A', 'I K', 'I K,leak', 'I M', 'I h', 'I Cl,Ca', 'I K,Ca', 'I CNG', 'I CAN', 'I Sodium', 'I Calcium', \"\n",
    "    f\"'I Mixed', 'I Potassium', 'I A, slow', 'ATP-sensitive potassium current', 'I_KHT', 'I_KLT', 'I_HERG', \"\n",
    "    f\"'Late Na', 'Na/Ca exchanger', 'I_Na,Ca', 'I_SERCA', 'KCNQ1', 'I_Ks', 'I Krp', 'I R', 'I Q', 'I_K,Na', \"\n",
    "    f\"'Na/K pump', 'I_AHP', 'I ANO2', 'I trp', 'I Cl, leak', 'I Na, leak', 'I Ca,p', 'I_KD', 'Osmosis-driven water flux', \"\n",
    "    f\"'KCC2', 'NKCC1', 'Ca pump', 'I_HCO3', 'Channelrhodopsin (ChR)', 'Kir', 'I MI', 'I TRPM8', 'Kir2 leak', \"\n",
    "    f\"'I Na, slow inactivation', 'Na+-glutamate transporter', 'IK Bkca', 'I Ca SOCC', 'IK Skca', 'KCC1', 'NBC', \"\n",
    "    f\"'I C', 'TASK channel', 'Kir, inactivating', 'I TRPM4'].\\n\\n\"\n",
    "    f\"Based on this content, please provide the most relevant ion current(s), just list them separated by commas, DO NOT analyze, \"\n",
    "    f\"This is an example: current 1, current 2,...\"\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    response_dict = chat_completion.to_dict()\n",
    "    metadata = response_dict[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    print(\"Generated metadata:\", metadata)\n",
    "\n",
    "    new_result = {\n",
    "        \"file_content\": file_content,\n",
    "        \"model_code\": code,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    new_prompt_block[\"results\"].append(new_result)\n",
    "\n",
    "with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(new_prompt_block, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files matching the rules have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/114665_mathched_file.txt\n",
      "Concatenated file have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/raw_file/114665_raw_file.txt\n",
      "Files matching the rules have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/118434_mathched_file.txt\n",
      "Concatenated file have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/raw_file/118434_raw_file.txt\n",
      "Files matching the rules have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/114424_mathched_file.txt\n",
      "Concatenated file have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/raw_file/114424_raw_file.txt\n",
      "Files matching the rules have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/105383_mathched_file.txt\n",
      "Concatenated file have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/raw_file/105383_raw_file.txt\n",
      "Files matching the rules have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/match_file/113949_mathched_file.txt\n",
      "Concatenated file have been saved to /Users/tessakong/Desktop/CodeAnalysis/sample5/raw_file/113949_raw_file.txt\n"
     ]
    }
   ],
   "source": [
    "def filter_model(model_name, json_file_path, sample_folder):\n",
    "    extract_folder = f'{sample_folder}/{model_name}'\n",
    "    output_file_folder = f'{sample_folder}/match_file'\n",
    "    os.makedirs(output_file_folder, exist_ok=True)\n",
    "    # Load rules from the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        rules = json.load(json_file)\n",
    "\n",
    "    # Convert rules to a dictionary of regex patterns and replacements\n",
    "    pattern_mapping = {re.compile(pattern): replacement for pattern, replacement in rules.items()}\n",
    "\n",
    "    matched_files = [] \n",
    "    # Define acceptable file extensions\n",
    "    acceptable_extensions = ('.py', '.cpp', '.java', '.m', '.txt', '.h', '.data', '.html', '.c', '.mod', '.g', '.p')  # Adjust as needed\n",
    "\n",
    "    # Traverse subfolders and files in the extraction folder\n",
    "    for subfolder in os.listdir(extract_folder):\n",
    "        subfolder_path = os.path.join(extract_folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):  # Check if it's a directory\n",
    "            for root, _, files in os.walk(subfolder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    # Check if the file extension is acceptable\n",
    "                    if not file.lower().endswith(acceptable_extensions):\n",
    "                        continue  # Skip the file if the extension is not acceptable\n",
    "\n",
    "                    # Open the file and check its content against the rules\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        for pattern, replacement_list in pattern_mapping.items():\n",
    "                            if pattern.search(content):  # If any rule matches the file content\n",
    "                                matched_files.append(file_path)\n",
    "                                break  # Stop checking this file if one rule is matched\n",
    "\n",
    "    # Write matched file contents to a new text file\n",
    "    output_file_path = f'{output_file_folder}/{model_name}_mathched_file.txt'\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for file_path in matched_files:\n",
    "            output_file.write(f'=== {file_path} ===\\n')  # Write the file path\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                output_file.write(f.read())  # Write the file content\n",
    "                output_file.write('\\n\\n')  # Add a newline between files\n",
    "\n",
    "    print(f\"Files matching the rules have been saved to {output_file_path}\")\n",
    "\n",
    "\n",
    "def concat_model(model_name, sample_folder):\n",
    "    extract_folder = f'{sample_folder}/{model_name}'\n",
    "    output_file_folder = f'{sample_folder}/raw_file'\n",
    "    os.makedirs(output_file_folder, exist_ok=True)\n",
    "\n",
    "    matched_files = []\n",
    "    # Define acceptable file extensions\n",
    "    acceptable_extensions = ('.py', '.cpp', '.java', '.m', '.txt', '.h', '.data', '.html', '.c', '.mod', '.g', '.p')  # Adjust as needed\n",
    "\n",
    "    # Traverse subfolders and files in the extraction folder\n",
    "    for subfolder in os.listdir(extract_folder):\n",
    "        subfolder_path = os.path.join(extract_folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):  # Check if it's a directory\n",
    "            for root, _, files in os.walk(subfolder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    # Check if the file extension is acceptable\n",
    "                    if not file.lower().endswith(acceptable_extensions):\n",
    "                        continue  # Skip the file if the extension is not acceptable\n",
    "                    matched_files.append(file_path)\n",
    "\n",
    "    # Write matched file contents to a new text file\n",
    "    output_file_path = f'{output_file_folder}/{model_name}_raw_file.txt'\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for file_path in matched_files:\n",
    "            output_file.write(f'=== {file_path} ===\\n')  # Write the file path\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                output_file.write(f.read())  # Write the file content\n",
    "                output_file.write('\\n\\n')  # Add a newline between files\n",
    "\n",
    "    print(f\"Concatenated file have been saved to {output_file_path}\")\n",
    "\n",
    "json_file_path = \"/Users/tessakong/Desktop/CodeAnalysis/manual_classifier_rules.json\"\n",
    "for code in file_code_list[:5]:\n",
    "    file_id = file_code_id[code]\n",
    "    filter_model(code, json_file_path, sample_folder)\n",
    "    concat_model(code,sample_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgNnJuPWWgoW",
    "outputId": "89f79b2b-6df2-482a-8e26-f397963e653e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "zip_file_path = '/content/drive/MyDrive/group capstone/modeldb_model/100603.zip'\n",
    "extract_folder = '/content/drive/MyDrive/group capstone/modeldb_model/100603/'\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "print(\"completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvqKyjjxdKSf",
    "outputId": "001a829a-fe52-4ebc-b07b-031c8a026f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files matching the rules have been saved to /content/drive/My Drive/group capstone/modeldb_model/match_file/100603_matched_files.txt\n"
     ]
    }
   ],
   "source": [
    "# Set the path for the JSON file and the output file\n",
    "json_file_path = '/content/drive/My Drive/group capstone/manual_classifier_rules.json'\n",
    "output_file_path = '/content/drive/My Drive/group capstone/modeldb_model/match_file/100603_matched_files.txt'\n",
    "\n",
    "# Load rules from the JSON file\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    rules = json.load(json_file)\n",
    "\n",
    "# Convert rules to a list of regex patterns and corresponding replacements\n",
    "pattern_mapping = {re.compile(pattern): replacement for pattern, replacement in rules.items()}\n",
    "\n",
    "matched_files = []\n",
    "# Set the list of acceptable file extensions\n",
    "acceptable_extensions = ('.py', '.cpp', '.java', '.m', '.txt','.h','.data','.html','.c')#不确定是否要限制文件类型\n",
    "\n",
    "# Traverse subfolders in the extract_folder directory\n",
    "for subfolder in os.listdir(extract_folder):\n",
    "    subfolder_path = os.path.join(extract_folder, subfolder)\n",
    "    if os.path.isdir(subfolder_path):  # Check if it's a directory\n",
    "        for root, _, files in os.walk(subfolder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Check if the file extension is acceptable\n",
    "                if not file.lower().endswith(acceptable_extensions):\n",
    "                    continue  # Skip the file if the extension is not acceptable\n",
    "\n",
    "                # Open the file and check its content against the rules\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    for pattern, replacement_list in pattern_mapping.items():\n",
    "                        if pattern.search(content):  # If any pattern matches the file content\n",
    "                            matched_files.append(file_path)\n",
    "                            break  # Stop checking this file if one rule is matched\n",
    "\n",
    "\n",
    "# Write matched file contents to a new text file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for file_path in matched_files:\n",
    "        output_file.write(f'=== {file_path} ===\\n')  # Write the file path\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            output_file.write(f.read())  # Write the file content\n",
    "            output_file.write('\\n\\n')  # Add a newline between files\n",
    "\n",
    "print(f\"Files matching the rules have been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8jimD5CM00o",
    "outputId": "01eb2169-ed32-40d8-fb47-a37bbf07c930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeldb-model  modeldb-zips\n"
     ]
    }
   ],
   "source": [
    "! ls '/content/drive/MyDrive/group capstone/modeldb_model/modeldb-zips'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PIotW2CL5Qv"
   },
   "outputs": [],
   "source": [
    "def process_model(model_name):\n",
    "    # Set the path for the zip file and extraction folder based on the model name\n",
    "    zip_file_path = f'/content/drive/MyDrive/group capstone/modeldb_model/modeldb-zips/modeldb-zips/{model_name}.zip'\n",
    "    extract_folder = f'/content/drive/MyDrive/group capstone/modeldb_model/modeldb-zips/modeldb-model/{model_name}/'\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_folder)\n",
    "\n",
    "    print(f\"{model_name} extraction completed\")\n",
    "\n",
    "    # Set the path for the JSON rules file and output file\n",
    "    json_file_path = '/content/drive/My Drive/group capstone/manual_classifier_rules.json'\n",
    "    output_file_path = f'/content/drive/My Drive/group capstone/modeldb_model/match_file/{model_name}_matched_files.txt'\n",
    "\n",
    "    # Load rules from the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        rules = json.load(json_file)\n",
    "\n",
    "    # Convert rules to a dictionary of regex patterns and replacements\n",
    "    pattern_mapping = {re.compile(pattern): replacement for pattern, replacement in rules.items()}\n",
    "\n",
    "    matched_files = []\n",
    "    # Define acceptable file extensions\n",
    "    acceptable_extensions = ('.py', '.cpp', '.java', '.m', '.txt', '.h', '.data', '.html', '.c')  # Adjust as needed\n",
    "\n",
    "    # Traverse subfolders and files in the extraction folder\n",
    "    for subfolder in os.listdir(extract_folder):\n",
    "        subfolder_path = os.path.join(extract_folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):  # Check if it's a directory\n",
    "            for root, _, files in os.walk(subfolder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    # Check if the file extension is acceptable\n",
    "                    if not file.lower().endswith(acceptable_extensions):\n",
    "                        continue  # Skip the file if the extension is not acceptable\n",
    "\n",
    "                    # Open the file and check its content against the rules\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        for pattern, replacement_list in pattern_mapping.items():\n",
    "                            if pattern.search(content):  # If any rule matches the file content\n",
    "                                matched_files.append(file_path)\n",
    "                                break  # Stop checking this file if one rule is matched\n",
    "\n",
    "    # Write matched file contents to a new text file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for file_path in matched_files:\n",
    "            output_file.write(f'=== {file_path} ===\\n')  # Write the file path\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                output_file.write(f.read())  # Write the file content\n",
    "                output_file.write('\\n\\n')  # Add a newline between files\n",
    "\n",
    "    print(f\"Files matching the rules have been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pod_Mt1VOQFn",
    "outputId": "60de42e4-d653-49d9-d0ab-295d74a7c6b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93394 extraction completed\n",
      "Files matching the rules have been saved to /content/drive/My Drive/group capstone/modeldb_model/match_file/93394_matched_files.txt\n"
     ]
    }
   ],
   "source": [
    "model_name = '93394'\n",
    "process_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1Aovv2v_jhD"
   },
   "outputs": [],
   "source": [
    "api_key = os.getenv('API_KEY')\n",
    "organization=os.getenv('ORGANIZATION')\n",
    "output_file_path='/content/drive/My Drive/group capstone/modeldb_model/match_file/93394_matched_files.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BnsTFUBo_Ux1",
    "outputId": "ac5d6e90-f274-493e-87d6-89fddf90df53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated metadata: Metadata:\n",
      "- Author: Quentin Huys\n",
      "- Year: 2006\n",
      "- Title: Fast population coding Neural Computation\n",
      "- Description: Example code for fast population coding with sparse spike trains in conjunction with a 2006 paper by Quentin Huys, Zemel RS, Natarajan R and Dayan P.\n",
      "- License: GNU General Public License\n",
      "- Website: http://www.gatsby.ucl.ac.uk/~qhuys/code.html\n",
      "- Email: qhuys@gatsby.ucl.ac.uk\n",
      "- Files:\n",
      "  - COPYRIGHT.txt\n",
      "  - getinf.m\n",
      "  - getspk.m\n",
      "  - getstim.m\n",
      "  - LICENSE.txt\n",
      "  - main.m\n",
      "  - param.m\n",
      "  - plots.m\n",
      "  - psinf.m\n",
      "  - pspred.m\n",
      "  - README.txt\n",
      "  - setup.m\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_path, 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    organization=organization\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please analyze the following content and provide some metadata about it:\\n\\n{file_content}\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "response_dict = chat_completion.to_dict()\n",
    "metadata = response_dict[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "print(\"Generated metadata:\", metadata)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "codeanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
